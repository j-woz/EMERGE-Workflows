
= Agent Workflow via Shell Invocation (agent-sh)

This directory contains several workflows for running the ExaEpi `agent` .

== Build

These notes are how to compile everything from scratch.  Users can skip down to <<Installations>>.

. Load module `cudatoolkit`
. Compile a plain MPICH, called `$MPICH`

. Insert this line:
+
----
set(CMAKE_CUDA_FLAGS "-I$MPICH/include")
----
+
into both:
+
.. `ExaEpi/CMakeLists.txt`
.. `ExaEpi/build/_deps/amrex-src/CMakeLists.txt`
. Run `cmake` for ExaEpi with:
+
----
$ cmake
    -DCMAKE_C_COMPILER=$MPICH/bin/mpicc
    -DCMAKE_CXX_COMPILER=$MPICH/bin/mpicxx
    -DAMReX_GPU_BACKEND=CUDA
----
+
. Then:
+
----
$ make
----
. If you change `MPICH` or anything else, you must delete the CMake cache files
+
----
$ rm -r CMakeCache.txt CMakeFiles
----

== Installations

=== Perlmutter

These are encoded in `settings.sh` .

* MPICH:  `/global/cfs/cdirs/m5071/sfw/mpich-4.3.0`
* ExaEpi: `/global/cfs/cdirs/m5071/sfw/ExaEpi-mpich`
** Data: A path-consistent copy of the example data is here too
* Swift/T: `/global/cfs/cdirs/m5071/sfw/swift-t`
** This is built with modules `PrgEnv-gnu` and `tensorflow/2.15.0` (for Python)

== User setup

Even when using the software from <<Installations>>, the workflow user must:

. Clone the ExaEpi repo `git@github.com:AMReX-Agent/ExaEpi.git`
. Clone this repo `git@github.com:j-woz/EMERGE-Workflows.git`
. Enter the NERSC Jupyter environment at https://jupyter.nersc.gov
.. Load the kernel "HPC Bootcamp 2025" (from "No kernel", upper right of window)
.. Run the input-generator script from `EMERGE-Workflows/notebooks/input_gen.ipynb`
. Run the desired workflow below from <<Workflows>>

=== Input generator script: `input_gen.ipynb`

* This notebook generates a directory of inputs
* Each generated input file has a unique random number generator `seed` and `p_trans` combination
* You can easily edit the number of `seeds` and the list of `p_trans_choices` .

== System configurations

=== Perlmutter

Want to run 1 multi-rank-GPU ExaEpi per node, using 4 processes, 4 GPUs.

==== Rank/GPU configuration

. Swift/T is built to use the system MPI
. ExaEpi is built with MPICH so that it can only do intra-node communication
. Swift/T is configured to run 1 worker process (a pilot job) per node via `PPN=1`
. Our `agent.swift` is hard-coded to run the `agent` in a 4-rank MPI job

== Workflows

This information should be portable across systems.

=== Single-shot

. Runs the agent once for debugging
. Edit `one-shot-submit.sh` to set the data location
. The workflow is in `one-shot.swift`
. Run with:
+
----
$ ./one-shot-submit.sh
----
+
. This submits a job, reports the `JOB_ID`, and creates a workflow output directory `TURBINE_OUTPUT` (which can be customized)
. The `stdout` stream from the workflow goes to `TURBINE_OUTPUT/output.txt`
. The output/error streams from the agent go to `TURBINE_OUTPUT/agent-{out,err}.txt`, as specified in `one-shot.swift`

=== Loop

. Runs the agent several times in a loop to test simple sweeps
. Edit `loop-submit.sh` to set the data locations
. The workflow is in `loop.swift`
.. Each input file provided on the command line to this script will be run
.. Swift/T loops like this are concurrent.  The tasks produced may execute in any order, on any available resources
. Run with:
+
----
$ ./loop-submit.sh
----
+
. Output is as for the `one-shot` workflow, except:
.. The output/error streams from the agent go to `TURBINE_OUTPUT/{output,errors}-{i}.txt`, for each loop iteration `i`, as specified in `loop.swift`

=== Loop-Glob

Runs the agent several times in a loop, once for each input file in a given directory

. Set up a directory with your inputs as described above, for example:
+
----
$ INPUTS=$HOME/input-data-001
----
. Run:
+
----
$ ./loop-glob-submit.sh $INPUTS
----
. At runtime, a hook `copy-inputs.sh` copies the `$INPUTS` to the `inputs` directory in the workflow output, and uses that location for the inputs.
. The workflow reports the new `TURBINE_OUTPUT` directory.  This contains all workflow outputs
.
. To change the scheduler settings (node count, walltime, etc.), edit `settings.sh` .
