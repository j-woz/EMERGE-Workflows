
= Agent Workflow via Shell Invocation (agent-sh)

== Build

These notes are how to compile everything from scratch.  Users can skip down to <<Installations>>.

. Compile a plain MPICH, called `$MPICH`

. Insert this line:
+
----
set(CMAKE_CUDA_FLAGS "-I$MPICH/include")
----
+
into both:
+
.. `ExaEpi/CMakeLists.txt`
.. `ExaEpi/build/_deps/amrex-src/CMakeLists.txt`
. Run `cmake` for ExaEpi with:
+
----
$ cmake
    -DCMAKE_C_COMPILER=$MPICH/bin/mpicc
    -DCMAKE_CXX_COMPILER=$MPICH/bin/mpicxx
    -DAMReX_GPU_BACKEND=CUDA
    -D
----
+
. Then:
+
----
$ make
----

== Installations

=== Perlmutter

These are encoded in `settings.sh` .

* MPICH:  `/global/cfs/cdirs/m5071/sfw/mpich-4.3.0`
* ExaEpi: `/global/cfs/cdirs/m5071/sfw/ExaEpi-mpich`
** Data: A path-consistent copy of the example data is here too
* Swift/T: `/global/cfs/cdirs/m5071/sfw/swift-t`
** This is built with modules `PrgEnv-gnu` and `tensorflow/2.15.0` (for Python)

== System configurations

=== Perlmutter

Want to run 1 multi-rank-GPU ExaEpi per node, using 4 processes, 4 GPUs.

==== Rank/GPU configuration

. Swift/T is built to use the system MPI
. ExaEpi is built with MPICH so that it can only do intra-node communication
. Swift/T is configured to run 1 worker process (a pilot job) per node via `PPN=1`
. Our `agent.swift` is hard-coded to run the `agent` in a 4-rank MPI job

== Workflows

This information should be portable across systems.

=== Single-shot

. Runs the agent once for debugging
. Edit `one-shot-submit.sh` to set the data location
. The workflow is in `one-shot.swift`
. Run with:
+
----
$ ./one-shot-submit.sh
----
+
. This submits a job, reports the `JOB_ID`, and creates a workflow output directory `TURBINE_OUTPUT` (which can be customized)
. The `stdout` stream from the workflow goes to `TURBINE_OUTPUT/output.txt`
. The output/error streams from the agent go to `TURBINE_OUTPUT/agent-{out,err}.txt`, as specified in `one-shot.swift`

=== Loop

. Runs the agent several times in a loop to test simple sweeps
. Edit `loop-submit.sh` to set the data locations
. The workflow is in `loop.swift`
.. Each input file provided on the command line to this script will be run
.. Swift/T loops like this are concurrent.  The tasks produced may execute in any order, on any available resources
. Run with:
+
----
$ ./loop-submit.sh
----
+
. Output is as for the `one-shot` workflow, except:
.. The output/error streams from the agent go to `TURBINE_OUTPUT/{output,errors}-{i}.txt`, for each loop iteration `i`, as specified in `loop.swift`

=== Loop-Glob

. Runs the agent several times in a loop to test simple sweeps
. Set up a `DATA` directory:
+
----
WILL NOT WORK FOR USERS!!!
$ DATA=/global/cfs/cdirs/m5071/sfw/ExaEpi-mpich/data
----
. Copy:
+
----
$ cp ../data-sets/Feb1.cases $DATA/CaseData
----
. Set up an `INPUTS` directory, an absolute path:
+
----
$ INPUTS=$PWD/../run-001-inputs
----
. Run:
+
----
$ ./loop-glob-submit.sh $DATA $INPUTS
----
. To change the data file names, edit the `params[]` entries in `loop-glob.swift`
